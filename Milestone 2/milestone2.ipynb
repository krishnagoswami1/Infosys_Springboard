{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting Up the Environment\n",
    "\n",
    "This first cell handles all the necessary setup. It installs the required Python libraries for running the language models (`transformers`, `torch`, `bitsandbytes`), data manipulation (`pandas`), and code analysis (`radon`). After installation, it imports all the modules that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Section 1: Setup & Installations ---\n",
    "print(\"Installing required libraries...\")\n",
    "# Install all necessary packages quietly (-q flag)\n",
    "!pip install transformers torch accelerate bitsandbytes pandas huggingface_hub radon ipywidgets matplotlib seaborn -q\n",
    "\n",
    "# Import necessary modules\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "import ast # Abstract Syntax Trees, used to check for valid Python code\n",
    "import re # Regular expressions for cleaning text\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from radon.complexity import cc_visit # For Cyclomatic Complexity\n",
    "from radon.metrics import mi_visit # For Maintainability Index\n",
    "from radon.raw import analyze # For Lines of Code (LOC)\n",
    "\n",
    "print(\"\\nSetup Complete! âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hugging Face Authentication\n",
    "\n",
    "To download the pre-trained models from the Hugging Face Hub, we need to authenticate. This section retrieves a secret key (stored securely in Kaggle Secrets) and uses it to log in. This step is crucial for accessing gated models or private repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import the secret client to access stored secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Retrieve the Hugging Face API key\n",
    "secret_value = user_secrets.get_secret(\"HUGGINGFACE_KEY\")\n",
    "\n",
    "# Print a confirmation message without exposing the full key\n",
    "print(secret_value[:5]+'*************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use the retrieved key to log into the Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "login(token = secret_value)\n",
    "print(\"Huggingface Login successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Core Engine: Generation and Metrics\n",
    "\n",
    "This cell contains the backbone of our analysis tool. It defines:\n",
    "1.  **Models to Test**: A dictionary mapping user-friendly names to their Hugging Face model paths.\n",
    "2.  **Helper Functions**:\n",
    "    * `clean_generated_code`: Each model has a unique way of formatting its output (e.g., special tokens, instructions). This function uses regular expressions to strip away the noise and extract only the raw Python code.\n",
    "    * `is_syntactically_valid`: Checks if the generated code can be parsed, ensuring it's valid Python syntax before we try to analyze it.\n",
    "    * `calculate_advanced_metrics`: Uses the `radon` library to compute key software metrics: Cyclomatic Complexity (decision complexity), Maintainability Index (ease of maintenance), and Lines of Code.\n",
    "    * `generate_code`: The main generation function. It formats the prompt for the specific model, generates code, measures the generation time, and then cleans the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Model Configuration ---\n",
    "\n",
    "# Dictionary mapping friendly names to Hugging Face model identifiers\n",
    "MODELS_TO_TEST = {\n",
    "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
    "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
    "    \"Stable-code-3b\": \"stabilityai/stable-code-3b\"\n",
    "}\n",
    "# Set the computation device to GPU (cuda) if available, otherwise CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Helper & Generation Functions ---\n",
    "def clean_generated_code(text, model_path):\n",
    "    \"\"\"Cleans the raw model output to extract only the Python code.\"\"\"\n",
    "    model_path = model_path.lower()\n",
    "    # Model-specific cleaning logic\n",
    "    if \"gemma\" in model_path:\n",
    "        text = re.sub(r\"<start_of_turn>user\\n.*<end_of_turn>\\n<start_of_turn>model\\n\", \"\", text, flags=re.DOTALL).replace(\"<end_of_turn>\", \"\")\n",
    "    elif \"phi-2\" in model_path:\n",
    "        text = re.sub(r\"Instruct:.*\\nOutput:\", \"\", text, flags=re.DOTALL)\n",
    "        text = text.replace(\"<|endoftext|>\", \"\")\n",
    "    elif \"stable\" in model_path:\n",
    "        text = re.sub(r'\"\"\".*?\"\"\"\\s*', \"\", text, flags=re.DOTALL) # Remove docstrings\n",
    "        match = re.search(r\"```(?:python)?\\n(.*?)\\n```\", text, re.DOTALL) # Extract from markdown block\n",
    "        if match:\n",
    "            text = match.group(1)\n",
    "        text = re.sub(r\"<\\|?endoftext\\|?>\", \"\", text, flags=re.IGNORECASE)\n",
    "    else: # Default cleaning for instruction-tuned models\n",
    "        text = re.sub(r\"### Instruction:\\n.*\\n\\n### Response:\", \"\", text, flags=re.DOTALL)\n",
    "    \n",
    "    # General cleaning for code within markdown blocks\n",
    "    match = re.search(r\"```python\\n(.*?)\\n```\", text, re.DOTALL)\n",
    "    if match: text = match.group(1)\n",
    "    return text.strip()\n",
    "\n",
    "def is_syntactically_valid(code_string: str) -> bool:\n",
    "    \"\"\"Checks if a string contains valid Python code using AST parsing.\"\"\"\n",
    "    if not code_string: return False\n",
    "    try:\n",
    "        ast.parse(code_string)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "\n",
    "def calculate_advanced_metrics(code_string):\n",
    "    \"\"\"Calculates code quality metrics if the code is syntactically valid.\"\"\"\n",
    "    if not is_syntactically_valid(code_string):\n",
    "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
    "    try:\n",
    "        # Calculate Cyclomatic Complexity, Maintainability Index, and Lines of Code\n",
    "        complexity = sum([c.complexity for c in cc_visit(code_string)]) if cc_visit(code_string) else 0\n",
    "        maintainability = mi_visit(code_string, multi=True)\n",
    "        loc = analyze(code_string).loc\n",
    "        return {\"complexity\": complexity, \"maintainability\": round(float(maintainability), 2), \"loc\": loc}\n",
    "    except Exception:\n",
    "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
    "\n",
    "def generate_code(model, tokenizer, prompt):\n",
    "    \"\"\"Generates code from a model, times the process, and cleans the output.\"\"\"\n",
    "    model_path = model.name_or_path.lower()\n",
    "    \n",
    "    # Apply model-specific prompt formatting\n",
    "    if \"gemma\" in model_path:\n",
    "        formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    elif \"phi-2\" in model_path:\n",
    "        formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
    "    elif \"stable\" in model_path:\n",
    "        formatted_prompt = f'\"\"\"\\n{prompt}\\n\"\"\"\\n'\n",
    "    else: # Default instruction format\n",
    "        formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the prompt and move it to the correct device (GPU/CPU)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Generate code and time the inference\n",
    "    start_time = time.time()\n",
    "    output_ids = model.generate(\n",
    "        inputs.input_ids, \n",
    "        attention_mask=inputs.attention_mask, \n",
    "        max_new_tokens=512, \n",
    "        temperature=0.1, # Low temperature for more deterministic, less creative output\n",
    "        do_sample=True, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Decode the generated token IDs back to a string\n",
    "    raw_output = tokenizer.batch_decode(output_ids)[0]\n",
    "    # Clean the raw output to get just the code\n",
    "    cleaned_code = clean_generated_code(raw_output, model_path)\n",
    "\n",
    "    return {\"code\": cleaned_code, \"gen_time\": end_time - start_time}\n",
    "\n",
    "print(\"Backend engine with advanced metrics is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pre-loading Models\n",
    "\n",
    "Loading a large language model into memory can be time-consuming. To make the interactive UIs feel responsive, this cell pre-loads all the models and their corresponding tokenizers defined in `MODELS_TO_TEST`. They are stored in a dictionary (`loaded_models`) for quick access later. Using `torch_dtype=torch.bfloat16` and `device_map=\"auto\"` helps optimize memory usage and automatically places the model on the available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Section 4: Pre-Loading All Models ---\n",
    "loaded_models = {}\n",
    "print(\"Starting to pre-load all models...\")\n",
    "\n",
    "# Iterate through the dictionary of models to test\n",
    "for model_name, model_path in MODELS_TO_TEST.items():\n",
    "    if model_name not in loaded_models.keys():\n",
    "        print(f\"\\n--- Loading {model_name}... ---\")\n",
    "        try:\n",
    "            # Load the tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "            # Load the model with optimizations for memory and device placement\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path, \n",
    "                torch_dtype=torch.bfloat16, # Use bfloat16 for reduced memory footprint\n",
    "                device_map=\"auto\", # Automatically map model layers to available devices (GPU/CPU)\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            # Store the loaded model and tokenizer\n",
    "            loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "            print(f\"âœ… {model_name} loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— FAILED to load {model_name}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nAll available models are pre-loaded.\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is likely for debugging or verification, simply printing the names (keys) of the models that were successfully loaded into the `loaded_models` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Verify which models have been loaded by checking the dictionary keys\n",
    "loaded_models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows an attempt to load an additional model (`replit-code-v1-3b`). The output shows that it failed, which is useful for demonstrating that not all models are compatible with the current setup out-of-the-box. The error is caught gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Attempt to load a different model to test compatibility\n",
    "model_path = \"replit/replit-code-v1-3b\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
    "    loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "    print(f\"âœ… {model_name} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    # Catch and print any errors during the loading process\n",
    "    print(\"Failed to load Replit\", e )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interactive UI #1: Benchmark All Models\n",
    "\n",
    "This section builds the first user interface using `ipywidgets`. It provides a simple way to test a single prompt across all the pre-loaded models simultaneously.\n",
    "\n",
    "1.  **UI Elements**: A text area for the user's prompt and a button to trigger the generation.\n",
    "2.  **Button Logic**: The `on_benchmark_all_clicked` function is attached to the button. When clicked, it takes the prompt, iterates through all models, calls the `generate_code` function for each, calculates metrics, and displays the results neatly in a table (Pandas DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the text input area for the prompt\n",
    "prompt_input_1 = widgets.Textarea(\n",
    "    placeholder=\"Enter your code prompt here (e.g., 'a function to calculate the factorial of a number')\",\n",
    "    layout={'width': '90%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "# Create the button to start the benchmark\n",
    "generate_button_1 = widgets.Button(description=\"Generate & Benchmark All\")\n",
    "\n",
    "# Create an output area to display results\n",
    "output_1 = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Define the function that runs when the 'Generate & Benchmark All' button is clicked\n",
    "def on_benchmark_all_clicked(b):\n",
    "    with output_1:\n",
    "        clear_output() # Clear previous results\n",
    "        prompt = prompt_input_1.value\n",
    "        if not prompt:\n",
    "            print(\"Please enter a prompt.\")\n",
    "            return\n",
    "\n",
    "        print(f\"ðŸš€ Starting benchmark for prompt: '{prompt}'\\n\" + \"=\"*50)\n",
    "        results_this_run = []\n",
    "        # Loop through each pre-loaded model\n",
    "        for name, model_id in MODELS_TO_TEST.items():\n",
    "            print(f\"\\n--- Generating with {name} ---\")\n",
    "            # Generate code using the model's components\n",
    "            generated_code = generate_code(loaded_models[name]['model'],loaded_models[name]['tokenizer'], prompt)\n",
    "            # Calculate quality metrics for the generated code\n",
    "            metrics = calculate_advanced_metrics(generated_code['code'])\n",
    "            # Combine all results into a single dictionary entry\n",
    "            entry = {'Model': name, 'Prompt':prompt, **generated_code, **metrics}\n",
    "            results_this_run.append(entry)\n",
    "            \n",
    "        # Convert the list of results into a Pandas DataFrame for nice formatting\n",
    "        results_df = pd.DataFrame(results_this_run).round(2)\n",
    "        # Display the DataFrame as an HTML table\n",
    "        display(HTML(results_df.to_html().replace('\\\\n','<br>')))\n",
    "\n",
    "# Attach the function to the button's click event\n",
    "generate_button_1.on_click(on_benchmark_all_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display the UI components together\n",
    "print(\"UI #1: Benchmark All Models\")\n",
    "display(widgets.VBox([prompt_input_1, generate_button_1, output_1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interactive UI #2: Inspect Selected Models\n",
    "\n",
    "This section builds a second, more flexible UI. Instead of running on all models, the user can select specific models to compare using checkboxes. This is useful for focusing the analysis on a subset of models that seem promising.\n",
    "\n",
    "The structure is similar to the first UI, with `ipywidgets` for the prompt, checkboxes, a button, and an output area. The `on_run_selected_clicked` function gathers input only from the checked models before running the generation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Section 6: UI #2 - Run on Selected Models ---\n",
    "print(\"\\n\\n--- UI #2: Inspect Selected Models ---\")\n",
    "\n",
    "# Define the UI widgets\n",
    "prompt_input_selected = widgets.Textarea(placeholder='Enter a prompt for selected models...', layout={'width': '95%'})\n",
    "run_selected_button = widgets.Button(description='Run Selected', button_style='success', icon='play')\n",
    "output_selected = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
    "\n",
    "# Create a checkbox for each loaded model\n",
    "model_checkboxes = {name: widgets.Checkbox(value=True, description=name) for name in loaded_models.keys()}\n",
    "checkbox_container = widgets.VBox(list(model_checkboxes.values()))\n",
    "\n",
    "# Define the function that runs when the 'Run Selected' button is clicked\n",
    "def on_run_selected_clicked(b):\n",
    "    with output_selected:\n",
    "        output_selected.clear_output(wait=True)\n",
    "        prompt = prompt_input_selected.value\n",
    "        if not prompt: print(\"Please enter a prompt.\"); return\n",
    "\n",
    "        # Get the list of models that have been checked by the user\n",
    "        models_to_run = [name for name, cb in model_checkboxes.items() if cb.value]\n",
    "        if not models_to_run: print(\"Please select at least one model.\"); return\n",
    "\n",
    "        print(f\"Running prompt on {len(models_to_run)} selected models...\")\n",
    "        results_this_run = []\n",
    "        # Loop only through the selected models\n",
    "        for model_name in models_to_run:\n",
    "            print(f\"  - Generating with {model_name}...\")\n",
    "            components = loaded_models[model_name]\n",
    "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
    "            metrics = calculate_advanced_metrics(result['code'])\n",
    "\n",
    "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
    "            results_this_run.append(entry)\n",
    "\n",
    "        print(\"\\n--- Selected Run Complete ---\")\n",
    "        results_df = pd.DataFrame(results_this_run).round(2)\n",
    "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
    "\n",
    "# Attach the function to the button's click event\n",
    "run_selected_button.on_click(on_run_selected_clicked)\n",
    "\n",
    "# Assemble and display the UI\n",
    "ui_selected_models = widgets.VBox([prompt_input_selected, widgets.HTML(\"<h4>Select models to run:</h4>\"), checkbox_container, run_selected_button, output_selected])\n",
    "display(ui_selected_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Automated Testing and Visualization\n",
    "\n",
    "This final section automates the entire evaluation process to provide a high-level, objective comparison of the models. It runs a predefined list of 16 common coding prompts against every model.\n",
    "\n",
    "After collecting data for all prompts and models, it aggregates the results and generates four bar plots using `matplotlib` and `seaborn` to visualize the average performance of each model across several key metrics:\n",
    "- **Cyclomatic Complexity**: How complex is the generated code? (Lower is better)\n",
    "- **Maintainability Index**: How easy is the code to maintain? (Higher is better)\n",
    "- **Lines of Code (LOC)**: How verbose is the code? (Context-dependent, but often lower is better)\n",
    "- **Generation Time**: How fast is the model? (Lower is better)\n",
    "\n",
    "This provides a comprehensive, at-a-glance summary of each model's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# A list of standard prompts to test the models against automatically\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Python function is_palindrome(s) that returns True if a string is a palindrome, ignoring case and non-alphanumeric characters.\",\n",
    "    \"Write a Python function find_common_elements(list1, list2) that returns a new list containing elements that are present in both input lists.\",\n",
    "    \"Implement a Stack class in Python with push, pop, peek, and is_empty methods.\",\n",
    "    \"Write a Python function get_unique_even_numbers(numbers) that takes a list of integers and returns a sorted list of unique even numbers, using a list comprehension.\",\n",
    "    \"Write a Python function merge_dictionaries(d1, d2) that merges two dictionaries. If a key exists in both, the value from the second dictionary should overwrite the first.\",\n",
    "    \"Write a Python function count_words_in_file(filepath) that reads a text file and returns the total number of words.\",\n",
    "    \"Write a Python function get_bitcoin_price() that uses the requests library to fetch the current Bitcoin price from the CoinDesk API (https://api.coindesk.com/v1/bpi/currentprice.json) and returns the price in USD as a float.\",\n",
    "    \"A function that takes a list of numbers and returns the sum.\",\n",
    "    \"Implement a binary search algorithm.\",\n",
    "    \"A function to find the factorial of a number using recursion.\",\n",
    "    \"Write a simple Flask route that returns 'Hello, World!'.\",\n",
    "    \"A function to parse a date string 'YYYY-MM-DD' and return a datetime object.\",\n",
    "    \"A Python class for a 'Car' with 'make', 'model', and 'year' attributes.\",\n",
    "    \"A function to read a CSV file using pandas and return a DataFrame.\",\n",
    "    \"A function to write a dictionary to a JSON file.\",\n",
    "    \"A function that uses regex to validate an email address.\"\n",
    "]\n",
    "\n",
    "# Define UI widgets for the automated test section\n",
    "results = []\n",
    "output_3 = widgets.Output()\n",
    "run_test_button = widgets.Button(description=\"Run Full Test & Generate Plots\")\n",
    "\n",
    "# Define the main function for automated testing and plotting\n",
    "def run_tests_and_plot(b):\n",
    "    global results\n",
    "    results = [] # Reset results for a new run\n",
    "    with output_3:\n",
    "        clear_output()\n",
    "        print(\"Starting automated testing across all prompts and models...\")\n",
    "\n",
    "        # Outer loop: iterate through each test prompt\n",
    "        for i, prompt in enumerate(TEST_PROMPTS):\n",
    "            print(f\"\\nRunning Prompt {i+1}/{len(TEST_PROMPTS)}: '{prompt}'\")\n",
    "            # Inner loop: iterate through each model\n",
    "            for name, model_id in MODELS_TO_TEST.items():\n",
    "                print(f\"  - Generating with {name}...\")\n",
    "                output_gen_code = generate_code(loaded_models[name]['model'],loaded_models[name]['tokenizer'], prompt)\n",
    "                code = output_gen_code['code']\n",
    "                gen_time = output_gen_code['gen_time']\n",
    "                print('generation time is : ', gen_time)\n",
    "                metrics = calculate_advanced_metrics(code)\n",
    "                # Only append results if metrics could be calculated (i.e., code was valid)\n",
    "                if \"error\" not in metrics and metrics['complexity'] is not None:\n",
    "                    results.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"model\": name,\n",
    "                        \"generation_time\": gen_time,\n",
    "                        **metrics\n",
    "                    })\n",
    "        print(\"\\nâœ… Automated testing complete.\")\n",
    "        df = pd.DataFrame(results)\n",
    "        if df.empty:\n",
    "            print(\"No results to plot.\")\n",
    "            return\n",
    "\n",
    "        # --- Visualization ---\n",
    "        print(\"\\nðŸ“Š Generating performance plots...\")\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(12, 24))\n",
    "        fig.suptitle('Model Performance Metrics Across All Prompts', fontsize=16)\n",
    "\n",
    "        # Plot 1: Average Cyclomatic Complexity (Lower is Better)\n",
    "        avg_complexity = df.groupby('model')['complexity'].mean().sort_values()\n",
    "        std_complexity = df.groupby('model')['complexity'].std().reindex(avg_complexity.index)\n",
    "        avg_complexity.plot(kind='bar', ax=axes[0], color='skyblue', yerr=std_complexity, capsize=4)\n",
    "        axes[0].set_title('Average Cyclomatic Complexity (Lower is Better)')\n",
    "        axes[0].set_ylabel('Avg. Complexity')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Plot 2: Average Maintainability Index (Higher is Better)\n",
    "        avg_mi = df.groupby('model')['maintainability'].mean().sort_values(ascending=False)\n",
    "        std_mi = df.groupby('model')['maintainability'].std().reindex(avg_mi.index)\n",
    "        avg_mi.plot(kind='bar', ax=axes[1], color='lightgreen', yerr=std_mi, capsize=4)\n",
    "        axes[1].set_title('Average Maintainability Index (Higher is Better)')\n",
    "        axes[1].set_ylabel('Avg. MI')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Plot 3: Average Lines of Code (LOC)\n",
    "        avg_loc = df.groupby('model')['loc'].mean().sort_values()\n",
    "        std_loc = df.groupby('model')['loc'].std().reindex(avg_loc.index)\n",
    "        avg_loc.plot(kind='bar', ax=axes[2], color='salmon', yerr=std_loc, capsize=4)\n",
    "        axes[2].set_title('Average Lines of Code (LOC)')\n",
    "        axes[2].set_ylabel('Avg. LOC')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Plot 4: Average Generation Time (Lower is Better)\n",
    "        avg_time = df.groupby('model')['generation_time'].mean().sort_values()\n",
    "        std_time = df.groupby('model')['generation_time'].std().reindex(avg_time.index)\n",
    "        avg_time.plot(kind='bar', ax=axes[3], color='purple', yerr=std_time, capsize=4)\n",
    "        axes[3].set_title('Average Generation Time (Lower is Better)')\n",
    "        axes[3].set_ylabel('Avg. Time (seconds)')\n",
    "        axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to prevent title overlap\n",
    "        plt.show()\n",
    "\n",
    "# Link button to function\n",
    "run_test_button.on_click(run_tests_and_plot)\n",
    "\n",
    "# Display UI\n",
    "print(\"\\n\\nAutomated Testing & Visualization\")\n",
    "display(widgets.VBox([run_test_button, output_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ]
}
