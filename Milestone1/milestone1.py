# -*- coding: utf-8 -*-
"""milestone1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bPvaCo9NJJWmfuQ3Nq0q0fDcGphuc5Uw

# **Setup and Imports**
"""

import ast
import io
import tokenize
import keyword
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from collections import Counter
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity

"""# Code Snippets for analysis"""

# Objective: Prepare a minimum of 10 code snippets.
code_snippets = [
    # Snippet 1:
    "def add(a, b):\n    return a + b",

    # Snippet 2:
    "class Dog:\n    def __init__(self, name):\n        self.name = name",

    # Snippet 3:
    "squares = [x*x for x in range(10)]",

    # Snippet 4:
    "with open('file.txt', 'w') as f:\n    f.write('Hello NLP')",

    # Snippet 5:
    "from collections import Counter \nimport pandas as pd\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})",

    # Snippet 6:
    "def find_max(numbers):\n    max_val = numbers[0]\n    for num in numbers:\n        if num > max_val:\n            max_val = num\n    return max_val",

    # Snippet 7:
    "class Greeter:\n    def say_hello(self, person_name):\n        print(f'Hello, {person_name}!')",

    # Snippet 8:
    "for i in range(5):\n    print(i)",

    # Snippet 9:
    "try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print('Cannot divide by zero!')",

    # Snippet 10:
    "def is_even(number):\n    return number % 2 == 0"
]

"""# Parse AST"""

def extract_ast_features(code):
  tree = ast.parse(code)
  features = {
      'functions':[],
      'classes': [],
      'imports':[],
      'patterns':[]
  }
  for node in ast.walk(tree):
    if isinstance(node, ast.FunctionDef):
      features['functions'].append(node.name)
    elif isinstance(node, ast.ClassDef):
      features['classes'].append(node.name)
    elif isinstance(node, ast.Import):
            features["imports"].extend([alias.name for alias in node.names])
    elif isinstance(node, ast.ImportFrom):
        features["imports"].extend([alias.name for alias in node.names])
    elif isinstance(node, ast.Try):
        if "Exception Handling" not in features["patterns"]:
            features["patterns"].append("Exception Handling")
    elif isinstance(node, ast.With):
        if "With Statement" not in features["patterns"]:
            features["patterns"].append("With Statement")
    elif isinstance(node, ast.For):
        if "For Loop" not in features["patterns"]:
            features["patterns"].append("For Loop")

  return features

extract_ast_features(code_snippets[3])

"""# Tokenization"""

def tokenize_code(code):
  tokens = []
  try:
    generator = tokenize.generate_tokens(io.StringIO(code).readline)
    for token in generator:
      if token.type not in {tokenize.COMMENT, tokenize.NL, tokenize.NEWLINE, tokenize.INDENT, tokenize.DEDENT, tokenize.ENDMARKER}:
        tokens.append(token.string)
  except tokenize.TokenError:
    return ['parsing error']
  return tokens

tokenize_code(code_snippets[3])

processed_data = []
for i, snippet in enumerate(code_snippets):
    ast_features = extract_ast_features(snippet)
    tokens = tokenize_code(snippet)
    processed_data.append({
        "id": f"Snippet {i+1}",
        "code": snippet,
        "features": ast_features,
        "tokens": tokens
    })

df_processed = pd.DataFrame(processed_data)

df_processed

for index, row in df_processed.iterrows():
    print(f"{row['id']}: {row['features']}")

"""# Embedding generation with models"""

model_names = {
    "MiniLM": "all-MiniLM-L6-v2",
    "DistilRoBERTa": "all-distilroberta-v1", # Removed due to error
    "MPNet": "all-mpnet-base-v2"
}
models = {}
embeddings = {}

print("\n--- Loading Models and Generating Embeddings ---")
for name, path in model_names.items():
    print(f"Loading {name} model...")
    models[name] = SentenceTransformer(path, trust_remote_code=True)
    print(f"Generating embeddings with {name}...")
    embeddings[name] = models[name].encode(df_processed['code'].tolist())
    print(f"[âœ“] Embeddings generated for {name}.")

"""# Model Comparison"""

pca = PCA(n_components=2)
fig, axes = plt.subplots(1, len(models), figsize=(20, 6))
fig.suptitle('2D Visualization of Code Snippet Embeddings by Model', fontsize=16)

for i, (name, embs) in enumerate(embeddings.items()):
    embeddings_2d = pca.fit_transform(embs)
    ax = axes[i]
    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
    for j, row in enumerate(embeddings_2d):
        ax.annotate(f"{j+1}", (row[0], row[1]), textcoords="offset points", xytext=(0,5), ha='center')
    ax.set_title(name)
    ax.set_xlabel("PCA Component 1")
    ax.set_ylabel("PCA Component 2")
    ax.grid(True)

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

print("\n--- Visualization Analysis ---")
print("The plots show how each model clusters the code snippets based on semantic meaning.")
print("Now, consider how the presence of specific 'patterns' might influence these clusters.")
print("For example, does a model group Snippet 3 (List Comp) and Snippet 8 (For Loop) together because they are both iterative?")

"""Heatmaps"""



fig_heatmap, axes_heatmap = plt.subplots(1, len(models), figsize=(22, 6))
fig_heatmap.suptitle('Pairwise Code Similarity Heatmaps by Model', fontsize=16)
snippet_labels = [f"{i+1}" for i in range(len(code_snippets))]

for i, (name, embs) in enumerate(embeddings.items()):
    # Calculate cosine similarity matrix
    similarity_matrix = cosine_similarity(embs)

    # Plot heatmap
    ax = axes_heatmap[i]
    sns.heatmap(similarity_matrix, ax=ax, annot=True, fmt=".2f", cmap="viridis",
                xticklabels=snippet_labels, yticklabels=snippet_labels)
    ax.set_title(f"{name} Similarity")
    ax.set_xlabel("Snippet ID")
    ax.set_ylabel("Snippet ID")

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

